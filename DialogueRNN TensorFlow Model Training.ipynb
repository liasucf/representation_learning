{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff6862ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import torch.nn as nn\n",
    "import numpy as np, pickle, time, argparse\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report, precision_recall_fscore_support\n",
    "\n",
    "import tensorflow as tf\n",
    "import contractions\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import time\n",
    "import tensorflow.keras as k\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9a26ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"deal with contracted texts\"\n",
    "def expand_text(text):\n",
    "    expanded_words = []\n",
    "    text = text.encode('utf-8').decode('cp1252').replace(\"Â’\", \"'\")\n",
    "    #text = text.replace(\"’\", \"'\")\n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shotened words\n",
    "      expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "\n",
    "\"clean dataset\"\n",
    "def preprocess_text(x):\n",
    "    for punct in '\"!&?.,}-/<>#$%\\()*+:;=?@[\\\\]^_`|\\~':\n",
    "        x = x.replace(punct, ' ')\n",
    "    x = ' '.join(x.split())\n",
    "    x = x.lower()\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def create_utterances(filename, split):\n",
    "    sentences, emotion_labels, speakers, conv_id, = [], [], [], []\n",
    "    \n",
    "    lengths = []\n",
    "    with open(filename, 'r', encoding='latin1') as f:\n",
    "        a = json.load(f)\n",
    "        for c_id, line in enumerate(a):\n",
    "            for item in line:\n",
    "                sentences.append(item['utterance'])\n",
    "                emotion_labels.append(item['emotion'])\n",
    "                conv_id.append(split[:2] + '_c' + str(c_id))\n",
    "                speakers.append(item['speaker'])\n",
    "            \n",
    "            # u_id += 1\n",
    "                \n",
    "    data = pd.DataFrame(sentences, columns=['sentence'])\n",
    "    data['sentence'] = data['sentence'].apply(lambda x: expand_text(x))\n",
    "    data['sentence'] = data['sentence'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "    data['emotion_label'] = emotion_labels\n",
    "    data['speaker'] = speakers\n",
    "    data['conv_id'] = conv_id\n",
    "\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a4e267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model, this can take some time...\n",
      "Completed loading pretrained GloVe model.\n",
      "Done. Completed preprocessing.\n"
     ]
    }
   ],
   "source": [
    "\"create embedding\"\n",
    "def load_pretrained_glove():\n",
    "    print(\"Loading GloVe model, this can take some time...\")\n",
    "    glv_vector = {}\n",
    "    f = open('glove.840B.300d.txt', encoding='utf-8')\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float')\n",
    "            glv_vector[word] = coefs\n",
    "        except ValueError:\n",
    "            continue\n",
    "    f.close()\n",
    "    print(\"Completed loading pretrained GloVe model.\")\n",
    "    return glv_vector\n",
    "\n",
    "def encode_labels(encoder, l):\n",
    "    return encoder[l]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    train_data = create_utterances('Friends/friends_train.json', 'train')\n",
    "    valid_data = create_utterances('Friends/friends_dev.json', 'valid')\n",
    "    test_data = create_utterances('Friends/friends_test.json', 'test')\n",
    "    \n",
    "    ## encode the emotion and dialog act labels ##\n",
    "    all_emotion_labels =  set(train_data['emotion_label'])\n",
    "    emotion_label_encoder, emotion_label_decoder = {}, {}\n",
    "\n",
    "\n",
    "    for i, label in enumerate(all_emotion_labels):\n",
    "        emotion_label_encoder[label] = i\n",
    "        emotion_label_decoder[i] = label\n",
    "\n",
    "\n",
    "    pickle.dump(emotion_label_encoder, open('emotion_label_encoder.pkl', 'wb'))\n",
    "    pickle.dump(emotion_label_decoder, open('emotion_label_decoder.pkl', 'wb'))\n",
    "\n",
    "    train_data['encoded_emotion_label'] = train_data['emotion_label'].map(lambda x: encode_labels(emotion_label_encoder, x))\n",
    "    test_data['encoded_emotion_label'] = test_data['emotion_label'].map(lambda x: encode_labels(emotion_label_encoder, x))\n",
    "    valid_data['encoded_emotion_label'] = valid_data['emotion_label'].map(lambda x: encode_labels(emotion_label_encoder, x))\n",
    "    \n",
    "    \n",
    "    ## tokenize all sentences ##\n",
    "    all_text = list(train_data['sentence'])\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_text)\n",
    "    pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "\n",
    "    ## convert the sentences into sequences ##\n",
    "    train_sequence = tokenizer.texts_to_sequences(list(train_data['sentence']))\n",
    "    valid_sequence = tokenizer.texts_to_sequences(list(valid_data['sentence']))\n",
    "    test_sequence = tokenizer.texts_to_sequences(list(test_data['sentence']))\n",
    "    \n",
    "    train_data['sentence_length'] = [len(item) for item in train_sequence]\n",
    "    valid_data['sentence_length'] = [len(item) for item in valid_sequence]\n",
    "    test_data['sentence_length'] = [len(item) for item in test_sequence]\n",
    "    \n",
    "    max_num_tokens = 250\n",
    "\n",
    "    train_sequence = pad_sequences(train_sequence, maxlen=max_num_tokens, padding='post')\n",
    "    valid_sequence = pad_sequences(valid_sequence, maxlen=max_num_tokens, padding='post')\n",
    "    test_sequence = pad_sequences(test_sequence, maxlen=max_num_tokens, padding='post')\n",
    "\n",
    "    train_data['sequence'] = list(train_sequence)\n",
    "    valid_data['sequence'] = list(valid_sequence)\n",
    "    test_data['sequence'] = list(test_sequence)\n",
    "    \n",
    "   \n",
    "    \n",
    "    ## save pretrained embedding matrix ##\n",
    "    glv_vector = load_pretrained_glove()\n",
    "    word_vector_length = len(glv_vector['the'])\n",
    "    word_index = tokenizer.word_index\n",
    "    inv_word_index = {v: k for k, v in word_index.items()}\n",
    "    num_unique_words = len(word_index)\n",
    "    glv_embedding_matrix = np.zeros((num_unique_words+1, word_vector_length))\n",
    "\n",
    "    for j in range(1, num_unique_words+1):\n",
    "        try:\n",
    "            glv_embedding_matrix[j] = glv_vector[inv_word_index[j]]\n",
    "        except KeyError:\n",
    "            glv_embedding_matrix[j] = np.random.randn(word_vector_length)/200\n",
    "\n",
    "    np.ndarray.dump(glv_embedding_matrix, open('glv_embedding_matrix', 'wb'))\n",
    "    print ('Done. Completed preprocessing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "814ac1e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['emotion_true'] = pd.get_dummies(train_data['encoded_emotion_label']).values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4ee2ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>speaker</th>\n",
       "      <th>conv_id</th>\n",
       "      <th>encoded_emotion_label</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sequence</th>\n",
       "      <th>emotion_true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>also i was the point person on my company's tr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>tr_c0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>[371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you must have had your hands full</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>tr_c0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>[2, 311, 17, 98, 44, 643, 760, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that i did that i did</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>tr_c0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>[9, 1, 48, 9, 1, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>so let us talk a little bit about your duties</td>\n",
       "      <td>neutral</td>\n",
       "      <td>The Interviewer</td>\n",
       "      <td>tr_c0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>[23, 84, 79, 175, 7, 100, 402, 54, 44, 1470, 0...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my duties all right</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>tr_c0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[26, 1470, 34, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>you or me</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>tr_c719</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>[2, 112, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10557</th>\n",
       "      <td>i got it uh joey women do not have adam's apples</td>\n",
       "      <td>non-neutral</td>\n",
       "      <td>Ross</td>\n",
       "      <td>tr_c719</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>[1, 57, 6, 51, 81, 316, 12, 8, 17, 2757, 5888,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10558</th>\n",
       "      <td>you guys are messing with me right</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Joey</td>\n",
       "      <td>tr_c719</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>[2, 87, 13, 2747, 37, 20, 36, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10559</th>\n",
       "      <td>yeah</td>\n",
       "      <td>neutral</td>\n",
       "      <td>All</td>\n",
       "      <td>tr_c719</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10560</th>\n",
       "      <td>that was a good one for a second there i was l...</td>\n",
       "      <td>non-neutral</td>\n",
       "      <td>Joey</td>\n",
       "      <td>tr_c719</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>[9, 31, 7, 77, 66, 35, 7, 234, 46, 1, 31, 45, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10561 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence emotion_label  \\\n",
       "0      also i was the point person on my company's tr...       neutral   \n",
       "1                      you must have had your hands full       neutral   \n",
       "2                                  that i did that i did       neutral   \n",
       "3          so let us talk a little bit about your duties       neutral   \n",
       "4                                    my duties all right      surprise   \n",
       "...                                                  ...           ...   \n",
       "10556                                          you or me       neutral   \n",
       "10557   i got it uh joey women do not have adam's apples   non-neutral   \n",
       "10558                 you guys are messing with me right      surprise   \n",
       "10559                                               yeah       neutral   \n",
       "10560  that was a good one for a second there i was l...   non-neutral   \n",
       "\n",
       "               speaker  conv_id  encoded_emotion_label  sentence_length  \\\n",
       "0             Chandler    tr_c0                      1               18   \n",
       "1      The Interviewer    tr_c0                      1                7   \n",
       "2             Chandler    tr_c0                      1                6   \n",
       "3      The Interviewer    tr_c0                      1               10   \n",
       "4             Chandler    tr_c0                      0                4   \n",
       "...                ...      ...                    ...              ...   \n",
       "10556         Chandler  tr_c719                      1                3   \n",
       "10557             Ross  tr_c719                      7               11   \n",
       "10558             Joey  tr_c719                      0                7   \n",
       "10559              All  tr_c719                      1                1   \n",
       "10560             Joey  tr_c719                      7               13   \n",
       "\n",
       "                                                sequence  \\\n",
       "0      [371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759, ...   \n",
       "1      [2, 311, 17, 98, 44, 643, 760, 0, 0, 0, 0, 0, ...   \n",
       "2      [9, 1, 48, 9, 1, 48, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3      [23, 84, 79, 175, 7, 100, 402, 54, 44, 1470, 0...   \n",
       "4      [26, 1470, 34, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                  ...   \n",
       "10556  [2, 112, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "10557  [1, 57, 6, 51, 81, 316, 12, 8, 17, 2757, 5888,...   \n",
       "10558  [2, 87, 13, 2747, 37, 20, 36, 0, 0, 0, 0, 0, 0...   \n",
       "10559  [24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "10560  [9, 31, 7, 77, 66, 35, 7, 234, 46, 1, 31, 45, ...   \n",
       "\n",
       "                   emotion_true  \n",
       "0      [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "1      [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "2      [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "3      [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "4      [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                         ...  \n",
       "10556  [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "10557  [0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "10558  [1, 0, 0, 0, 0, 0, 0, 0]  \n",
       "10559  [0, 1, 0, 0, 0, 0, 0, 0]  \n",
       "10560  [0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "\n",
       "[10561 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c2ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['sequence'] = np.array(train_data['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19f935c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_train_data = train_data.groupby(\"conv_id\").agg(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86f8bcb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>speaker</th>\n",
       "      <th>encoded_emotion_label</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sequence</th>\n",
       "      <th>emotion_true</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conv_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tr_c0</th>\n",
       "      <td>[also i was the point person on my company's t...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, ...</td>\n",
       "      <td>[18, 7, 6, 10, 4, 16, 2, 18, 3, 5, 7, 28, 1, 7...</td>\n",
       "      <td>[[371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759,...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c1</th>\n",
       "      <td>[hey mon, hey hey hey you want to hear somethi...</td>\n",
       "      <td>[neutral, neutral, joy, sadness, surprise, neu...</td>\n",
       "      <td>[Chandler, Monica, Chandler, Monica, Chandler,...</td>\n",
       "      <td>[1, 1, 3, 4, 0, 1, 7, 4, 1, 1, 3, 1, 7, 0, 0, ...</td>\n",
       "      <td>[2, 10, 3, 8, 2, 12, 10, 2, 5, 6, 2, 5, 4, 8, ...</td>\n",
       "      <td>[[28, 509, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c10</th>\n",
       "      <td>[go go go, oh yeah now everybody wants to be u...</td>\n",
       "      <td>[joy, joy, non-neutral, surprise, neutral, neu...</td>\n",
       "      <td>[Ross, Rachel, Phoebe, Monica, Phoebe, Ross]</td>\n",
       "      <td>[3, 3, 7, 0, 1, 1]</td>\n",
       "      <td>[3, 10, 1, 6, 9, 10]</td>\n",
       "      <td>[[43, 43, 43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c100</th>\n",
       "      <td>[ooh oh no i have to go i have a massage appoi...</td>\n",
       "      <td>[non-neutral, neutral, neutral, joy, non-neutr...</td>\n",
       "      <td>[Phoebe, Eric, Phoebe, Eric, Mona, Ross, Dr. G...</td>\n",
       "      <td>[7, 1, 1, 3, 7, 1, 5, 4]</td>\n",
       "      <td>[12, 13, 13, 7, 10, 6, 19, 10]</td>\n",
       "      <td>[[219, 11, 16, 1, 17, 4, 43, 1, 17, 7, 1328, 1...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c101</th>\n",
       "      <td>[okay so we will just stay married, yes exactl...</td>\n",
       "      <td>[joy, joy, joy, non-neutral, fear, non-neutral...</td>\n",
       "      <td>[Rachel, Ross, Rachel, Ross, Rachel, Ross, Rac...</td>\n",
       "      <td>[3, 3, 3, 7, 2, 7, 0, 1, 7, 3, 1, 4, 1, 1, 7, ...</td>\n",
       "      <td>[7, 2, 9, 19, 15, 12, 10, 9, 1, 27, 2, 5, 3, 2...</td>\n",
       "      <td>[[22, 23, 18, 41, 25, 254, 186, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c95</th>\n",
       "      <td>[hey joey you wanted to talk to me, i do not k...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[Tag, Joey, Tag, Joey, Tag, Joey]</td>\n",
       "      <td>[1, 1, 1, 1, 0, 3]</td>\n",
       "      <td>[8, 11, 6, 13, 1, 3]</td>\n",
       "      <td>[[28, 81, 2, 198, 4, 175, 4, 20, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c96</th>\n",
       "      <td>[oh danielle i was not expecting the machine g...</td>\n",
       "      <td>[surprise, neutral, non-neutral, non-neutral, ...</td>\n",
       "      <td>[Chandler, Monica, Chandler, Ross, Chandler, C...</td>\n",
       "      <td>[0, 1, 7, 7, 2, 2, 7, 0, 5, 6, 6, 6, 7, 5, 7, ...</td>\n",
       "      <td>[21, 2, 5, 5, 2, 12, 21, 12, 9, 6, 9, 9, 4, 7,...</td>\n",
       "      <td>[[11, 3176, 1, 31, 8, 1150, 5, 503, 136, 20, 7...</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c97</th>\n",
       "      <td>[y'know they say a watched pot never beeps, it...</td>\n",
       "      <td>[neutral, non-neutral, neutral, anger, neutral...</td>\n",
       "      <td>[Monica, Phoebe, Monica, Phoebe, Monica, Phoeb...</td>\n",
       "      <td>[1, 7, 1, 5, 1, 1, 1, 1, 1, 4, 4, 4, 4]</td>\n",
       "      <td>[8, 20, 12, 7, 6, 1, 6, 5, 2, 2, 2, 19, 26]</td>\n",
       "      <td>[[53, 63, 101, 7, 3180, 3181, 128, 2103, 0, 0,...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c98</th>\n",
       "      <td>[ok bye well monica's not coming it is just go...</td>\n",
       "      <td>[neutral, neutral, neutral, joy, non-neutral, ...</td>\n",
       "      <td>[Ross, Chandler, Ross, Chandler, Ross, Chandle...</td>\n",
       "      <td>[1, 1, 1, 3, 7, 7, 0, 6, 0, 1, 7, 7, 1]</td>\n",
       "      <td>[15, 14, 8, 21, 2, 2, 15, 9, 18, 1, 6, 8, 3]</td>\n",
       "      <td>[[113, 179, 30, 488, 8, 251, 6, 3, 25, 29, 4, ...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tr_c99</th>\n",
       "      <td>[ok so it is just because it was my table i ha...</td>\n",
       "      <td>[non-neutral, neutral, anger, non-neutral, non...</td>\n",
       "      <td>[Chandler, Joey, Chandler, Joey, Chandler, Joe...</td>\n",
       "      <td>[7, 1, 5, 7, 7, 0, 6, 1, 7, 1, 7, 1, 1, 7, 1, ...</td>\n",
       "      <td>[17, 4, 13, 6, 17, 4, 17, 8, 8, 1, 9, 2, 18, 5...</td>\n",
       "      <td>[[113, 23, 6, 3, 25, 89, 6, 31, 26, 512, 1, 17...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sentence  \\\n",
       "conv_id                                                      \n",
       "tr_c0    [also i was the point person on my company's t...   \n",
       "tr_c1    [hey mon, hey hey hey you want to hear somethi...   \n",
       "tr_c10   [go go go, oh yeah now everybody wants to be u...   \n",
       "tr_c100  [ooh oh no i have to go i have a massage appoi...   \n",
       "tr_c101  [okay so we will just stay married, yes exactl...   \n",
       "...                                                    ...   \n",
       "tr_c95   [hey joey you wanted to talk to me, i do not k...   \n",
       "tr_c96   [oh danielle i was not expecting the machine g...   \n",
       "tr_c97   [y'know they say a watched pot never beeps, it...   \n",
       "tr_c98   [ok bye well monica's not coming it is just go...   \n",
       "tr_c99   [ok so it is just because it was my table i ha...   \n",
       "\n",
       "                                             emotion_label  \\\n",
       "conv_id                                                      \n",
       "tr_c0    [neutral, neutral, neutral, neutral, surprise,...   \n",
       "tr_c1    [neutral, neutral, joy, sadness, surprise, neu...   \n",
       "tr_c10   [joy, joy, non-neutral, surprise, neutral, neu...   \n",
       "tr_c100  [non-neutral, neutral, neutral, joy, non-neutr...   \n",
       "tr_c101  [joy, joy, joy, non-neutral, fear, non-neutral...   \n",
       "...                                                    ...   \n",
       "tr_c95   [neutral, neutral, neutral, neutral, surprise,...   \n",
       "tr_c96   [surprise, neutral, non-neutral, non-neutral, ...   \n",
       "tr_c97   [neutral, non-neutral, neutral, anger, neutral...   \n",
       "tr_c98   [neutral, neutral, neutral, joy, non-neutral, ...   \n",
       "tr_c99   [non-neutral, neutral, anger, non-neutral, non...   \n",
       "\n",
       "                                                   speaker  \\\n",
       "conv_id                                                      \n",
       "tr_c0    [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "tr_c1    [Chandler, Monica, Chandler, Monica, Chandler,...   \n",
       "tr_c10        [Ross, Rachel, Phoebe, Monica, Phoebe, Ross]   \n",
       "tr_c100  [Phoebe, Eric, Phoebe, Eric, Mona, Ross, Dr. G...   \n",
       "tr_c101  [Rachel, Ross, Rachel, Ross, Rachel, Ross, Rac...   \n",
       "...                                                    ...   \n",
       "tr_c95                   [Tag, Joey, Tag, Joey, Tag, Joey]   \n",
       "tr_c96   [Chandler, Monica, Chandler, Ross, Chandler, C...   \n",
       "tr_c97   [Monica, Phoebe, Monica, Phoebe, Monica, Phoeb...   \n",
       "tr_c98   [Ross, Chandler, Ross, Chandler, Ross, Chandle...   \n",
       "tr_c99   [Chandler, Joey, Chandler, Joey, Chandler, Joe...   \n",
       "\n",
       "                                     encoded_emotion_label  \\\n",
       "conv_id                                                      \n",
       "tr_c0    [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, ...   \n",
       "tr_c1    [1, 1, 3, 4, 0, 1, 7, 4, 1, 1, 3, 1, 7, 0, 0, ...   \n",
       "tr_c10                                  [3, 3, 7, 0, 1, 1]   \n",
       "tr_c100                           [7, 1, 1, 3, 7, 1, 5, 4]   \n",
       "tr_c101  [3, 3, 3, 7, 2, 7, 0, 1, 7, 3, 1, 4, 1, 1, 7, ...   \n",
       "...                                                    ...   \n",
       "tr_c95                                  [1, 1, 1, 1, 0, 3]   \n",
       "tr_c96   [0, 1, 7, 7, 2, 2, 7, 0, 5, 6, 6, 6, 7, 5, 7, ...   \n",
       "tr_c97             [1, 7, 1, 5, 1, 1, 1, 1, 1, 4, 4, 4, 4]   \n",
       "tr_c98             [1, 1, 1, 3, 7, 7, 0, 6, 0, 1, 7, 7, 1]   \n",
       "tr_c99   [7, 1, 5, 7, 7, 0, 6, 1, 7, 1, 7, 1, 1, 7, 1, ...   \n",
       "\n",
       "                                           sentence_length  \\\n",
       "conv_id                                                      \n",
       "tr_c0    [18, 7, 6, 10, 4, 16, 2, 18, 3, 5, 7, 28, 1, 7...   \n",
       "tr_c1    [2, 10, 3, 8, 2, 12, 10, 2, 5, 6, 2, 5, 4, 8, ...   \n",
       "tr_c10                                [3, 10, 1, 6, 9, 10]   \n",
       "tr_c100                     [12, 13, 13, 7, 10, 6, 19, 10]   \n",
       "tr_c101  [7, 2, 9, 19, 15, 12, 10, 9, 1, 27, 2, 5, 3, 2...   \n",
       "...                                                    ...   \n",
       "tr_c95                                [8, 11, 6, 13, 1, 3]   \n",
       "tr_c96   [21, 2, 5, 5, 2, 12, 21, 12, 9, 6, 9, 9, 4, 7,...   \n",
       "tr_c97         [8, 20, 12, 7, 6, 1, 6, 5, 2, 2, 2, 19, 26]   \n",
       "tr_c98        [15, 14, 8, 21, 2, 2, 15, 9, 18, 1, 6, 8, 3]   \n",
       "tr_c99   [17, 4, 13, 6, 17, 4, 17, 8, 8, 1, 9, 2, 18, 5...   \n",
       "\n",
       "                                                  sequence  \\\n",
       "conv_id                                                      \n",
       "tr_c0    [[371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759,...   \n",
       "tr_c1    [[28, 509, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "tr_c10   [[43, 43, 43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "tr_c100  [[219, 11, 16, 1, 17, 4, 43, 1, 17, 7, 1328, 1...   \n",
       "tr_c101  [[22, 23, 18, 41, 25, 254, 186, 0, 0, 0, 0, 0,...   \n",
       "...                                                    ...   \n",
       "tr_c95   [[28, 81, 2, 198, 4, 175, 4, 20, 0, 0, 0, 0, 0...   \n",
       "tr_c96   [[11, 3176, 1, 31, 8, 1150, 5, 503, 136, 20, 7...   \n",
       "tr_c97   [[53, 63, 101, 7, 3180, 3181, 128, 2103, 0, 0,...   \n",
       "tr_c98   [[113, 179, 30, 488, 8, 251, 6, 3, 25, 29, 4, ...   \n",
       "tr_c99   [[113, 23, 6, 3, 25, 89, 6, 31, 26, 512, 1, 17...   \n",
       "\n",
       "                                              emotion_true  \n",
       "conv_id                                                     \n",
       "tr_c0    [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...  \n",
       "tr_c1    [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...  \n",
       "tr_c10   [[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...  \n",
       "tr_c100  [[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, ...  \n",
       "tr_c101  [[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...  \n",
       "...                                                    ...  \n",
       "tr_c95   [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...  \n",
       "tr_c96   [[1, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...  \n",
       "tr_c97   [[0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, ...  \n",
       "tr_c98   [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...  \n",
       "tr_c99   [[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, ...  \n",
       "\n",
       "[720 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfdb411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelBinarizer:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lb = LabelBinarizer()\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Convert X to array\n",
    "        X = np.array(X)\n",
    "        # Fit X using the LabelBinarizer object\n",
    "        self.lb.fit(X)\n",
    "        # Save the classes\n",
    "        self.classes_ = self.lb.classes_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # Convert X to array\n",
    "        X = np.array(X)\n",
    "        # Fit + transform X using the LabelBinarizer object\n",
    "        Xlb = self.lb.fit_transform(X)\n",
    "        # Save the classes\n",
    "        self.classes_ = self.lb.classes_\n",
    "        if len(self.classes_) == 2:\n",
    "            Xlb = np.hstack((Xlb, 1 - Xlb))\n",
    "        return Xlb\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Convert X to array\n",
    "        X = np.array(X)\n",
    "        # Transform X using the LabelBinarizer object\n",
    "        Xlb = self.lb.transform(X)\n",
    "        if len(self.classes_) == 2:\n",
    "            Xlb = np.hstack((Xlb, 1 - Xlb))\n",
    "        return Xlb\n",
    "\n",
    "    def inverse_transform(self, Xlb):\n",
    "        # Convert Xlb to array\n",
    "        Xlb = np.array(Xlb)\n",
    "        if len(self.classes_) == 2:\n",
    "            X = self.lb.inverse_transform(Xlb[:, 0])\n",
    "        else:\n",
    "            X = self.lb.inverse_transform(Xlb)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3798b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_train_data['encoded_speaker'] = dialogue_train_data['speaker'].apply(lambda s: MyLabelBinarizer().fit_transform(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea6bee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_train_data['sequence'] = dialogue_train_data['sequence'].apply(lambda s: np.array(np.array(s)))\n",
    "dialogue_train_data['encoded_emotion_label'] = dialogue_train_data['encoded_emotion_label'].apply(lambda s: np.array(np.array(s)))\n",
    "dialogue_train_data['encoded_speaker'] = dialogue_train_data['encoded_speaker'].apply(lambda s: np.array(np.array(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7112d739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dialogue_train_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2279554b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conv_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>speaker</th>\n",
       "      <th>encoded_emotion_label</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>sequence</th>\n",
       "      <th>emotion_true</th>\n",
       "      <th>encoded_speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr_c0</td>\n",
       "      <td>[also i was the point person on my company's t...</td>\n",
       "      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n",
       "      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, ...</td>\n",
       "      <td>[18, 7, 6, 10, 4, 16, 2, 18, 3, 5, 7, 28, 1, 7...</td>\n",
       "      <td>[[371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759,...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr_c1</td>\n",
       "      <td>[hey mon, hey hey hey you want to hear somethi...</td>\n",
       "      <td>[neutral, neutral, joy, sadness, surprise, neu...</td>\n",
       "      <td>[Chandler, Monica, Chandler, Monica, Chandler,...</td>\n",
       "      <td>[1, 1, 3, 4, 0, 1, 7, 4, 1, 1, 3, 1, 7, 0, 0, ...</td>\n",
       "      <td>[2, 10, 3, 8, 2, 12, 10, 2, 5, 6, 2, 5, 4, 8, ...</td>\n",
       "      <td>[[28, 509, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr_c10</td>\n",
       "      <td>[go go go, oh yeah now everybody wants to be u...</td>\n",
       "      <td>[joy, joy, non-neutral, surprise, neutral, neu...</td>\n",
       "      <td>[Ross, Rachel, Phoebe, Monica, Phoebe, Ross]</td>\n",
       "      <td>[3, 3, 7, 0, 1, 1]</td>\n",
       "      <td>[3, 10, 1, 6, 9, 10]</td>\n",
       "      <td>[[43, 43, 43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0], [1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr_c100</td>\n",
       "      <td>[ooh oh no i have to go i have a massage appoi...</td>\n",
       "      <td>[non-neutral, neutral, neutral, joy, non-neutr...</td>\n",
       "      <td>[Phoebe, Eric, Phoebe, Eric, Mona, Ross, Dr. G...</td>\n",
       "      <td>[7, 1, 1, 3, 7, 1, 5, 4]</td>\n",
       "      <td>[12, 13, 13, 7, 10, 6, 19, 10]</td>\n",
       "      <td>[[219, 11, 16, 1, 17, 4, 43, 1, 17, 7, 1328, 1...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr_c101</td>\n",
       "      <td>[okay so we will just stay married, yes exactl...</td>\n",
       "      <td>[joy, joy, joy, non-neutral, fear, non-neutral...</td>\n",
       "      <td>[Rachel, Ross, Rachel, Ross, Rachel, Ross, Rac...</td>\n",
       "      <td>[3, 3, 3, 7, 2, 7, 0, 1, 7, 3, 1, 4, 1, 1, 7, ...</td>\n",
       "      <td>[7, 2, 9, 19, 15, 12, 10, 9, 1, 27, 2, 5, 3, 2...</td>\n",
       "      <td>[[22, 23, 18, 41, 25, 254, 186, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>[[0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conv_id                                           sentence  \\\n",
       "0    tr_c0  [also i was the point person on my company's t...   \n",
       "1    tr_c1  [hey mon, hey hey hey you want to hear somethi...   \n",
       "2   tr_c10  [go go go, oh yeah now everybody wants to be u...   \n",
       "3  tr_c100  [ooh oh no i have to go i have a massage appoi...   \n",
       "4  tr_c101  [okay so we will just stay married, yes exactl...   \n",
       "\n",
       "                                       emotion_label  \\\n",
       "0  [neutral, neutral, neutral, neutral, surprise,...   \n",
       "1  [neutral, neutral, joy, sadness, surprise, neu...   \n",
       "2  [joy, joy, non-neutral, surprise, neutral, neu...   \n",
       "3  [non-neutral, neutral, neutral, joy, non-neutr...   \n",
       "4  [joy, joy, joy, non-neutral, fear, non-neutral...   \n",
       "\n",
       "                                             speaker  \\\n",
       "0  [Chandler, The Interviewer, Chandler, The Inte...   \n",
       "1  [Chandler, Monica, Chandler, Monica, Chandler,...   \n",
       "2       [Ross, Rachel, Phoebe, Monica, Phoebe, Ross]   \n",
       "3  [Phoebe, Eric, Phoebe, Eric, Mona, Ross, Dr. G...   \n",
       "4  [Rachel, Ross, Rachel, Ross, Rachel, Ross, Rac...   \n",
       "\n",
       "                               encoded_emotion_label  \\\n",
       "0  [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, ...   \n",
       "1  [1, 1, 3, 4, 0, 1, 7, 4, 1, 1, 3, 1, 7, 0, 0, ...   \n",
       "2                                 [3, 3, 7, 0, 1, 1]   \n",
       "3                           [7, 1, 1, 3, 7, 1, 5, 4]   \n",
       "4  [3, 3, 3, 7, 2, 7, 0, 1, 7, 3, 1, 4, 1, 1, 7, ...   \n",
       "\n",
       "                                     sentence_length  \\\n",
       "0  [18, 7, 6, 10, 4, 16, 2, 18, 3, 5, 7, 28, 1, 7...   \n",
       "1  [2, 10, 3, 8, 2, 12, 10, 2, 5, 6, 2, 5, 4, 8, ...   \n",
       "2                               [3, 10, 1, 6, 9, 10]   \n",
       "3                     [12, 13, 13, 7, 10, 6, 19, 10]   \n",
       "4  [7, 2, 9, 19, 15, 12, 10, 9, 1, 27, 2, 5, 3, 2...   \n",
       "\n",
       "                                            sequence  \\\n",
       "0  [[371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759,...   \n",
       "1  [[28, 509, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [[43, 43, 43, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "3  [[219, 11, 16, 1, 17, 4, 43, 1, 17, 7, 1328, 1...   \n",
       "4  [[22, 23, 18, 41, 25, 254, 186, 0, 0, 0, 0, 0,...   \n",
       "\n",
       "                                        emotion_true  \\\n",
       "0  [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...   \n",
       "1  [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...   \n",
       "2  [[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 1], [0, 1, 0, 0, 0, 0, ...   \n",
       "4  [[0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, ...   \n",
       "\n",
       "                                     encoded_speaker  \n",
       "0  [[1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0,...  \n",
       "1  [[1, 0, 0, 0], [0, 0, 1, 0], [1, 0, 0, 0], [0,...  \n",
       "2  [[0, 0, 0, 1], [0, 0, 1, 0], [0, 1, 0, 0], [1,...  \n",
       "3  [[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1...  \n",
       "4  [[0, 1], [1, 0], [0, 1], [1, 0], [0, 1], [1, 0...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0077f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_id                                                            tr_c247\n",
       "sentence                 [yes fran, i know what time it is but i am loo...\n",
       "emotion_label            [neutral, anger, non-neutral, non-neutral, neu...\n",
       "speaker                  [Chandler, Chandler, Chandler, Chandler, Chand...\n",
       "encoded_emotion_label                                [1, 5, 7, 7, 1, 3, 0]\n",
       "sentence_length                                    [2, 18, 4, 16, 3, 1, 1]\n",
       "sequence                 [[90, 3812, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "emotion_true             [[0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, ...\n",
       "encoded_speaker                        [[0], [0], [0], [0], [0], [0], [0]]\n",
       "Name: 165, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_train_data.iloc[165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb88eda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conv_id                                                              tr_c0\n",
       "sentence                 [also i was the point person on my company's t...\n",
       "emotion_label            [neutral, neutral, neutral, neutral, surprise,...\n",
       "speaker                  [Chandler, The Interviewer, Chandler, The Inte...\n",
       "encoded_emotion_label    [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 2, 1, 0, 1, 0, ...\n",
       "sentence_length          [18, 7, 6, 10, 4, 16, 2, 18, 3, 5, 7, 28, 1, 7...\n",
       "sequence                 [[371, 1, 31, 5, 695, 401, 33, 26, 2758, 2759,...\n",
       "emotion_true             [[0, 1, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, ...\n",
       "encoded_speaker          [[1, 0, 0, 0], [0, 0, 0, 1], [1, 0, 0, 0], [0,...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogue_train_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25944c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNExtractor(k.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, max_num_tokens,glv_embedding_matrix, filters, kernel_sizes, dropout):\n",
    "        super(CNNExtractor, self).__init__()\n",
    "                \n",
    "        self.embedding = k.layers.Embedding(input_dim=vocab_size, output_dim= embedding_dim, \n",
    "                                            input_length=max_num_tokens, weights = [glv_embedding_matrix])\n",
    "        self.convs1 = k.layers.Conv1D(filters, \n",
    "                                           kernel_sizes[0], \n",
    "                                           activation='relu')\n",
    "        self.convs2 = k.layers.Conv1D(filters, \n",
    "                                           kernel_sizes[1], \n",
    "                                           activation='relu')\n",
    "        self.convs3 = k.layers.Conv1D(filters, \n",
    "                                           kernel_sizes[2], \n",
    "                                           activation='relu')\n",
    "        \n",
    "        self.pooling = k.layers.GlobalMaxPooling1D()\n",
    "        self.concatanate = k.layers.Concatenate()\n",
    "        self.dropout = k.layers.Dropout(dropout)\n",
    "        self.dense = k.layers.Dense(output_size, input_shape=(len(kernel_sizes) * filters,), activation='relu')\n",
    "\n",
    "    def call(self, inputs):\n",
    "         # input size = (num_words = 250)\n",
    "         #in the original code the input is equals to (num_utt, batch, num_words)\n",
    "            \n",
    "        x = self.embedding(inputs) # x size = (num_words = 250, embedding = 300)\n",
    "        x = tf.expand_dims(x, axis=-1) # x size =  (num_words = 250, embedding = 300, num_utt * batch = 1)\n",
    "        x = tf.transpose(x, [2, 1, 0]) # x size =  (num_utt * batch = 1, embedding = 300,num_words = 250 )\n",
    "        \n",
    "    \n",
    "        conv1_x = self.pooling(self.convs1(x)) # conv1_x size =  (num_utt * batch = 1, 50 )\n",
    "        conv2_x = self.pooling(self.convs2(x)) # conv2_x size =  (num_utt * batch = 1, 50 )\n",
    "        conv3_x = self.pooling(self.convs3(x)) # conv3_x size =  (num_utt * batch = 1, 50 )\n",
    "        x = self.concatanate([conv1_x, conv2_x, conv3_x]) # x size =  (num_utt * batch = 1, 150 )\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        # x size =  (num_utt * batch = 1, output_size = 100 )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9cdc8029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalGRU(k.layers.Layer):\n",
    "    def __init__(self, D_g):\n",
    "        \n",
    "        super(GlobalGRU, self).__init__()\n",
    "        self.global_gru = k.layers.GRU(D_g,\n",
    "                                #return_sequences=True,\n",
    "                                #return_state=True,\n",
    "                                bias_initializer=\"ones\",\n",
    "                                dropout=0.1,\n",
    "                                recurrent_initializer='glorot_uniform')\n",
    "        self.dropout = k.layers.Dropout(0.5)\n",
    "\n",
    "# h_P_previous -> previous party state\n",
    "#t_r _> textual representation\n",
    "#h_G_previous _> previous global state\n",
    "    def call(self, t_r, h_P_previous, h_G_previous):\n",
    "        t_r_h_P = tf.concat([h_P_previous, t_r], axis=-1)\n",
    "        #Global state        \n",
    "        t_r_h_P = tf.expand_dims(t_r_h_P, axis=-1)\n",
    "        output = self.global_gru(t_r_h_P, initial_state=h_G_previous)\n",
    "        return self.dropout(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c84f600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartyGRU(k.layers.Layer):\n",
    "    def __init__(self, D_p):\n",
    "        super(PartyGRU, self).__init__()\n",
    "        \n",
    "        self.party_gru = k.layers.GRU(D_p,\n",
    "                                #return_sequences=True,\n",
    "                                #return_state=True,\n",
    "                                bias_initializer=\"ones\",\n",
    "                                dropout=0.1,\n",
    "                                recurrent_initializer='glorot_uniform')\n",
    "        self.dropout = k.layers.Dropout(0.5)\n",
    "\n",
    "#c_t -> current context \n",
    "#t_r -> textual representation\n",
    "#h_P_previous -> previous party state\n",
    "    def call(self, c_t, t_r, h_P_previous):\n",
    "\n",
    "        t_r_c_t = tf.concat([c_t, t_r], axis=-1)\n",
    "        t_r_c_t = tf.expand_dims(t_r_c_t, axis=-1)\n",
    "\n",
    "        return self.dropout(self.party_gru(t_r_c_t, initial_state=h_P_previous))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5d73096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionGRU(k.layers.Layer):\n",
    "    def __init__(self, D_e):\n",
    "        super(EmotionGRU, self).__init__()\n",
    "\n",
    "        self.emotion_gru = k.layers.GRU(D_e,\n",
    "                                bias_initializer=\"ones\",\n",
    "                                dropout=0.1,\n",
    "                                recurrent_initializer='glorot_uniform')\n",
    "        self.dropout = k.layers.Dropout(0.5)\n",
    "\n",
    "#h_E_previous -> previous emotion state\n",
    "#h_P -> current party state\n",
    "    def call(self, h_P, h_E_previous):\n",
    "        \n",
    "        h_P = tf.expand_dims(h_P, axis=-1)\n",
    "        return self.dropout(self.emotion_gru(h_P, initial_state=h_E_previous))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1d9f9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassificationDense(k.layers.Layer):\n",
    "    def __init__(self, D_c, n_classes):\n",
    "        super(EmotionClassificationDense, self).__init__()\n",
    "\n",
    "        self.classification = k.layers.Dense(2*D_c, activation=\"relu\")\n",
    "\n",
    "        self.y = k.layers.Dense(n_classes, activation=\"softmax\")\n",
    "\n",
    "        \n",
    "    def call(self, h_E):\n",
    "        output = self.classification(h_E)\n",
    "        return self.y(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "36ddd8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(k.layers.Layer):\n",
    "    def __init__(self, D_g):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.dense = k.layers.Dense(D_g)\n",
    "\n",
    "    def call(self, h_G_all, t_r):\n",
    "        H_g = np.array(h_G_all) #Hg = (1, 150, n_iterations)\n",
    "        \n",
    "        t_r =  self.dense(t_r)  #  (1, 1, 150)\n",
    "        t_r = tf.expand_dims(t_r, 1)\n",
    "        score = tf.matmul( t_r, H_g, transpose_b=True)\n",
    "        a_t = tf.nn.softmax(score, axis = 0) # 1, 1, 2\n",
    "        \n",
    "        a_t = tf.transpose(a_t, [1, 2, 0]) \n",
    "        aux = tf.transpose(H_g, [1,0,2]) # 1 , 2, 150\n",
    "        c_t = tf.matmul(a_t, aux) \n",
    "        return c_t[-1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "027e00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueRNN(k.Model):\n",
    "    def __init__(self, D_g, D_p, D_e, D_c, n_classes, vocab_size, embedding_dim, cnn_output_size ,\n",
    "                 max_num_tokens, glv_embedding_matrix, filters, kernel_sizes, dropout):\n",
    "        \n",
    "        super(DialogueRNN, self).__init__()\n",
    "        \n",
    "        self.D_g = D_g\n",
    "        self.D_p = D_p\n",
    "        self.D_e = D_e\n",
    "        self.D_c = D_c\n",
    "        \n",
    "        self.cnnTextualRepresentation = CNNExtractor(vocab_size, embedding_dim, cnn_output_size, \n",
    "                                            max_num_tokens,glv_embedding_matrix, filters, kernel_sizes, dropout)\n",
    "        \n",
    "        self.attention = AttentionBlock(D_g)\n",
    "        self.partyGRU = PartyGRU(D_p)\n",
    "        self.globalGRU = GlobalGRU(D_g)\n",
    "        self.emotionGRU = EmotionGRU(D_e)\n",
    "        \n",
    "        self.classificationDense = EmotionClassificationDense(D_c, n_classes)\n",
    "\n",
    "    #for each conversation \n",
    "    def call(self, messages, speakers):  \n",
    "        \n",
    "        number_of_speakers =  speakers.shape[1]\n",
    "\n",
    "        speakers_states = []\n",
    "        for i in range(number_of_speakers):\n",
    "        #For each speaker initialize HP\n",
    "            speakers_states.append(tf.zeros((1, self.D_p)))\n",
    "            \n",
    "        h_G = tf.zeros((1, self.D_g))\n",
    "        \n",
    "        h_G_all = []\n",
    "        h_G_all.append(h_G)\n",
    "        \n",
    "        y_pred_prob_all = []\n",
    "        y_pred_all = []\n",
    "        #initialize emotion states\n",
    "        h_E = tf.zeros((1, self.D_e))\n",
    "        \n",
    "        c = 0\n",
    "        #for each message \n",
    "        for message,speaker in zip(messages, speakers):\n",
    "            #print(f\"message {c}\")\n",
    "            #get the speaker id\n",
    "            t_r = self.cnnTextualRepresentation(message)\n",
    "\n",
    "            speakers_id = np.argmax(speaker)\n",
    "            \n",
    "            if len(h_G_all)==1:\n",
    "                c_t = tf.zeros((1, self.D_g))\n",
    "            else:\n",
    "                c_t = self.attention(h_G_all, t_r)\n",
    "                c_t = tf.squeeze(c_t)\n",
    "                c_t = tf.expand_dims(c_t, 1)\n",
    "                c_t = tf.transpose(c_t)\n",
    "            \n",
    "            h_P_previous = speakers_states[speakers_id] \n",
    "            h_G = self.globalGRU(t_r, h_P_previous ,h_G)\n",
    "            h_G_all.append(h_G)\n",
    "\n",
    "            h_P = self.partyGRU(c_t, t_r, h_P_previous)\n",
    "            speakers_states[speakers_id] = h_P\n",
    "\n",
    "            h_E =  self.emotionGRU(h_P, h_E)\n",
    "            y_pred_prob = self.classificationDense(h_E)\n",
    "            y_pred_prob =  tf.squeeze(y_pred_prob)\n",
    "            y_pred = np.argmax(y_pred_prob, axis=-1)\n",
    "\n",
    "            y_pred_prob_all.append(y_pred_prob)\n",
    "            y_pred_all.append(y_pred)\n",
    "            c = c+1\n",
    "\n",
    "        del speakers_states\n",
    "        del h_P_previous\n",
    "        del h_G\n",
    "        del h_G_all\n",
    "        del h_E\n",
    "        del t_r\n",
    "        del c_t\n",
    "        del h_P\n",
    "         \n",
    "\n",
    "        return y_pred_prob_all, y_pred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e174e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 12\n",
    "\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_c = 100\n",
    "\n",
    "n_classes=8\n",
    "glv_embedding_matrix = np.load(open('glv_embedding_matrix', 'rb') ,allow_pickle=True)\n",
    "vocab_size, embedding_dim = glv_embedding_matrix.shape\n",
    "cnn_output_size=100\n",
    "max_num_tokens = 250\n",
    "filters = 50\n",
    "kernel_sizes = [3,4,5]\n",
    "dropout = 0.5 \n",
    "\n",
    "X = np.column_stack((dialogue_train_data['encoded_speaker'],dialogue_train_data['sequence']))\n",
    "y = dialogue_train_data['emotion_true'].values\n",
    "\n",
    "\n",
    "model = DialogueRNN(D_g, D_p, D_e, D_c, n_classes, vocab_size, embedding_dim, cnn_output_size , \n",
    "        max_num_tokens, glv_embedding_matrix, filters, kernel_sizes, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "42acf355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DialogueRNN at 0x26104bd7a00>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0899444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DialogueRNN at 0x26104bd7a00>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=k.losses.CategoricalCrossentropy(from_logits=False, reduction='none'),\n",
    "              metrics=k.metrics.Accuracy())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b821979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation\n",
      "0\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 0, Initial Loss: 2.128605604171753\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 1,         Loss: 1.6653130054473877\n",
      "Conversation\n",
      "1\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 1, Initial Loss: 1.8392261266708374\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 2,         Loss: 1.8392261266708374\n",
      "Conversation\n",
      "2\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 2, Initial Loss: 1.9406753778457642\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 3,         Loss: 1.9406753778457642\n",
      "Conversation\n",
      "3\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 3, Initial Loss: 1.8990087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 4,         Loss: 1.8990087509155273\n",
      "Conversation\n",
      "4\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4, Initial Loss: 1.9406754970550537\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 5,         Loss: 1.9406754970550537\n",
      "Conversation\n",
      "5\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 5, Initial Loss: 1.9168659448623657\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 6,         Loss: 1.9168659448623657\n",
      "Conversation\n",
      "6\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 6, Initial Loss: 1.9558268785476685\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 7,         Loss: 1.9558268785476685\n",
      "Conversation\n",
      "7\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 7, Initial Loss: 1.8124703168869019\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 8,         Loss: 1.8124703168869019\n",
      "Conversation\n",
      "8\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 8, Initial Loss: 1.9406753778457642\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 9,         Loss: 1.9406753778457642\n",
      "Conversation\n",
      "9\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9, Initial Loss: 2.059723138809204\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 10,         Loss: 2.059723138809204\n",
      "Conversation\n",
      "10\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 10, Initial Loss: 2.1906754970550537\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 11,         Loss: 2.1906754970550537\n",
      "Conversation\n",
      "11\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 11, Initial Loss: 1.7476929426193237\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 12,         Loss: 1.7476929426193237\n",
      "Conversation\n",
      "12\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 12, Initial Loss: 1.6073421239852905\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 13,         Loss: 1.6073421239852905\n",
      "Conversation\n",
      "13\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 13, Initial Loss: 1.910372257232666\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 14,         Loss: 1.910372257232666\n",
      "Conversation\n",
      "14\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14, Initial Loss: 1.8124703168869019\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 15,         Loss: 1.8124703168869019\n",
      "Conversation\n",
      "15\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 15, Initial Loss: 1.8740087747573853\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 16,         Loss: 1.8740087747573853\n",
      "Conversation\n",
      "16\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 16, Initial Loss: 1.8649177551269531\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 17,         Loss: 1.8649177551269531\n",
      "Conversation\n",
      "17\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 17, Initial Loss: 1.5865087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 18,         Loss: 1.5865087509155273\n",
      "Conversation\n",
      "18\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 18, Initial Loss: 1.4615087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 19,         Loss: 1.4615087509155273\n",
      "Conversation\n",
      "19\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 19, Initial Loss: 1.8454372882843018\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 20,         Loss: 1.8454372882843018\n",
      "Conversation\n",
      "20\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 20, Initial Loss: 1.8573421239852905\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 21,         Loss: 1.8573421239852905\n",
      "Conversation\n",
      "21\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 21, Initial Loss: 1.7476929426193237\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 22,         Loss: 1.7476929426193237\n",
      "Conversation\n",
      "22\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 22, Initial Loss: 2.1830995082855225\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 23,         Loss: 2.1830995082855225\n",
      "Conversation\n",
      "23\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 23, Initial Loss: 2.0240087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 24,         Loss: 2.0240087509155273\n",
      "Conversation\n",
      "24\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 24, Initial Loss: 1.8529560565948486\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 25,         Loss: 1.8529560565948486\n",
      "Conversation\n",
      "25\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 25, Initial Loss: 1.9406753778457642\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 26,         Loss: 1.9406753778457642\n",
      "Conversation\n",
      "26\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 26, Initial Loss: 1.9406753778457642\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 27,         Loss: 1.9406753778457642\n",
      "Conversation\n",
      "27\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 27, Initial Loss: 1.9882944822311401\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 28,         Loss: 1.9882944822311401\n",
      "Conversation\n",
      "28\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 28, Initial Loss: 1.3649178743362427\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 29,         Loss: 1.3649178743362427\n",
      "Conversation\n",
      "29\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 29, Initial Loss: 2.0240087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 30,         Loss: 2.0240087509155273\n",
      "Conversation\n",
      "30\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 30, Initial Loss: 1.6740087270736694\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 31,         Loss: 1.6740087270736694\n",
      "Conversation\n",
      "31\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 31, Initial Loss: 1.8622441291809082\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 32,         Loss: 1.8622441291809082\n",
      "Conversation\n",
      "32\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 32, Initial Loss: 1.5740087032318115\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 33,         Loss: 1.5740087032318115\n",
      "Conversation\n",
      "33\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 33, Initial Loss: 1.8649177551269531\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 34,         Loss: 1.8649177551269531\n",
      "Conversation\n",
      "34\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 34, Initial Loss: 2.059723138809204\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 35,         Loss: 2.059723138809204\n",
      "Conversation\n",
      "35\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 35, Initial Loss: 1.8295644521713257\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 36,         Loss: 1.8295644521713257\n",
      "Conversation\n",
      "36\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 36, Initial Loss: 2.2740087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 37,         Loss: 2.2740087509155273\n",
      "Conversation\n",
      "37\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 37, Initial Loss: 1.8622441291809082\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 38,         Loss: 1.8622441291809082\n",
      "Conversation\n",
      "38\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n",
      "Step: 38, Initial Loss: 1.7740087509155273\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "Step: 39,         Loss: 1.7740087509155273\n",
      "Conversation\n",
      "39\n",
      "['dialogue_rnn_28/cnn_extractor_28/embedding_28/embeddings:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_84/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_85/bias:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/conv1d_86/bias:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/kernel:0', 'dialogue_rnn_28/cnn_extractor_28/dense_110/bias:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/recurrent_kernel:0', 'dialogue_rnn_28/party_gru_28/gru_82/gru_cell_82/bias:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/recurrent_kernel:0', 'dialogue_rnn_28/emotion_gru_27/gru_84/gru_cell_84/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_112/bias:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/kernel:0', 'dialogue_rnn_28/emotion_classification_dense_27/dense_113/bias:0', 'dialogue_rnn_28/attention_block_28/dense_111/kernel:0', 'dialogue_rnn_28/attention_block_28/dense_111/bias:0']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 39, Initial Loss: 1.5740087032318115\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/recurrent_kernel:0', 'dialogue_rnn_28/global_gru_27/gru_83/gru_cell_83/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    }
   ],
   "source": [
    "loss_object = k.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss(model, messages, speakers, y):\n",
    "  # training=training is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    y_pred_prob_all, y_pred_all = model(messages, speakers)\n",
    "\n",
    "    result = loss_object(y_true=y, y_pred=y_pred_prob_all)\n",
    "    return result\n",
    "\n",
    "def grad(model, messages, speakers, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, messages, speakers, y)\n",
    "        print([var.name for var in tape.watched_variables()])\n",
    "\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 12\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for r in range(dialogue_train_data.shape[0]):\n",
    "        print(\"Conversation\")\n",
    "        print(r)\n",
    "\n",
    "        data = dialogue_train_data.iloc[r]\n",
    "\n",
    "        messages = data[\"sequence\"]\n",
    "        speakers = data[\"encoded_speaker\"]\n",
    "        y = data[\"emotion_true\"]\n",
    "        y_all = data[\"encoded_emotion_label\"]\n",
    "\n",
    "        loss_value, grads = grad(model, messages, speakers,  y)\n",
    "        print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        \n",
    "        print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss(model, messages, speakers, y).numpy()))\n",
    "        \n",
    "        # Track progress\n",
    "        epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        # training=True is needed only if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        y_pred_prob_all, y_pred_all =  model( messages, speakers)\n",
    "        epoch_accuracy.update_state(y_all, y_pred_all)\n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3899fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n",
    "fig.suptitle('Training Metrics')\n",
    "\n",
    "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
    "axes[0].plot(train_loss_results)\n",
    "\n",
    "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
    "axes[1].set_xlabel(\"Epoch\", fontsize=14)\n",
    "axes[1].plot(train_accuracy_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8989675",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "ds_test_batch = ds_test.batch(10)\n",
    "\n",
    "for (x, y) in ds_test_batch:\n",
    "  # training=False is needed only if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  logits = model(x, training=False)\n",
    "  prediction = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "  test_accuracy(prediction, y)\n",
    "\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b27f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(messages, speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb1c3c6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'messages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model(\u001b[43mmessages\u001b[49m, speakers)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'messages' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b25cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = k.optimizers.Adam()\n",
    "loss_object = k.losses.CategoricalCrossentropy(from_logits=False, reduction='none')\n",
    "accuracy_object = k.metrics.Accuracy()\n",
    "\n",
    "def loss_function(real, y):\n",
    "    loss_ = loss_object(real, y)\n",
    "    loss_ = tf.reduce_mean(loss_)\n",
    "    return loss_ #\n",
    "\n",
    "def accuracy_score(real, y):\n",
    "    accuracy_object.reset_states()\n",
    "    accuracy_object.update_state(real, y)\n",
    "    return accuracy_object.result()\n",
    "\n",
    "\n",
    "def train_step(model, dialogue_train_data):\n",
    "    loss, acc = 0, 0    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for r in range(dialogue_train_data.shape[0]):\n",
    "            print(\"Conversation\")\n",
    "            print(r)\n",
    "\n",
    "            data = dialogue_train_data.iloc[r]\n",
    "\n",
    "            messages = data[\"sequence\"]\n",
    "            speakers = data[\"encoded_speaker\"]\n",
    "            y = data[\"emotion_true\"]\n",
    "            y_all = data[\"encoded_emotion_label\"]\n",
    "\n",
    "            loss = []\n",
    "            acc = []\n",
    "             \n",
    "            y_pred_prob_all, y_pred_all = model(messages, speakers)\n",
    "\n",
    "            y_true = np.argmax(y, axis=-1)        \n",
    "            loss += loss_function(y, y_pred_prob_all)\n",
    "            acc += accuracy_score(y, y_pred_prob_all)\n",
    "            \n",
    "            #print(acc)\n",
    "    batch_loss = loss / (i-1)\n",
    "    batch_acc = acc / (i-1)\n",
    "    variables = model.trainable_variables\n",
    "    \n",
    "    \n",
    "    #print([var.name for var in tape.watched_variables()])\n",
    "    gradients = tape.gradient(loss, variables,\n",
    "                              unconnected_gradients=tf.UnconnectedGradients.ZERO\n",
    "                             )\n",
    "    \n",
    "    #print(gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss, batch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40665c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_epochs = 12\n",
    "\n",
    "D_g = 150\n",
    "D_p = 150\n",
    "D_e = 100\n",
    "D_c = 100\n",
    "\n",
    "n_classes=8\n",
    "glv_embedding_matrix = np.load(open('glv_embedding_matrix', 'rb') ,allow_pickle=True)\n",
    "vocab_size, embedding_dim = glv_embedding_matrix.shape\n",
    "cnn_output_size=100\n",
    "max_num_tokens = 250\n",
    "filters = 50\n",
    "kernel_sizes = [3,4,5]\n",
    "dropout = 0.5 \n",
    "\n",
    "X = np.column_stack((dialogue_train_data['encoded_speaker'],dialogue_train_data['sequence']))\n",
    "y = dialogue_train_data['emotion_true'].values\n",
    "\n",
    "\n",
    "model = DialogueRNN(D_g, D_p, D_e, D_c, n_classes, vocab_size, embedding_dim, cnn_output_size , \n",
    "        max_num_tokens, glv_embedding_matrix, filters, kernel_sizes, dropout)\n",
    "    \n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"Number of epoch\")\n",
    "    print(epoch)\n",
    "\n",
    "    batch_loss, batch_acc = train_step(model, dialogue_train_data)\n",
    "    loss.append(batch_loss)\n",
    "    acc.append(batch_acc)\n",
    "\n",
    "    plt.plot(loss, color=\"blue\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()\n",
    "    plt.plot(acc, color=\"red\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "    print('Epoch %d Loss %.3f Accuracy %.2f' % (epoch + 1, batch_loss.numpy(), batch_acc.numpy()))\n",
    "    training_time = (time.time() - start) / 60\n",
    "    print(\"Training done in %d min (%d epochs with batches of %d)\" % (training_time, n_epochs, batch_size))\n",
    "    break\n",
    "#encoder.save_weights(\"encoder.weights\")\n",
    "#decoder.save_weights(\"decoder.weights\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
